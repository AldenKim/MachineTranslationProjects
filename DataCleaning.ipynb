{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MRMuUcAwEsb",
        "outputId": "1572aa8c-0cdb-465f-93a1-809ce25ea073"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-tmx in /usr/local/lib/python3.12/dist-packages (0.3.1)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from python-tmx) (5.4.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!pip install python-tmx\n",
        "import re\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "jlS_jpMixCqh"
      },
      "outputs": [],
      "source": [
        "import PythonTmx as tmx\n",
        "from datetime import UTC, datetime\n",
        "import lxml.etree as etree\n",
        "\n",
        "def read_tmx(filepath, target_lang=\"ko-KR\", limit = None):\n",
        "    tree = etree.parse(filepath, etree.XMLParser(encoding=\"utf-8\"))\n",
        "    root = tree.getroot()\n",
        "\n",
        "    tmx_obj = tmx.from_element(root)\n",
        "\n",
        "    source, target = [], []\n",
        "    count = 0\n",
        "\n",
        "    for tu in tmx_obj.tus:\n",
        "        if limit and count >= limit:\n",
        "          break\n",
        "\n",
        "        s, t = None, None\n",
        "        for tuv in tu.tuvs:\n",
        "            lang = tuv.lang.lower()\n",
        "            text = \"\".join([str(c) for c in tuv.content if isinstance(c, str)])\n",
        "            if lang.startswith(\"en\"):\n",
        "                s = text\n",
        "            elif lang.startswith(target_lang.lower()):\n",
        "                t = text\n",
        "        if s and t:\n",
        "            source.append(s)\n",
        "            target.append(t)\n",
        "            count+=1\n",
        "\n",
        "    return source, target\n",
        "\n",
        "file1 = \"/content/drive/MyDrive/Korean/14 320 - Korean - FamilyHistory - Recent.tmx\"\n",
        "source1, target1 = read_tmx(file1, limit=100000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "gKZN4Q_rh4O8"
      },
      "outputs": [],
      "source": [
        "file2 = \"/content/drive/MyDrive/Korean/14 320 - Korean - Legacy - 1.tmx\"\n",
        "source2, target2 = read_tmx(file2, limit=100000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file2 = \"/content/drive/MyDrive/Korean/14 320 - Korean - Recent.tmx\"\n",
        "source3, target3 = read_tmx(file2, limit=100000)\n",
        "full_source = source1 + source2 + source3\n",
        "full_target = target1 + target2 + target3"
      ],
      "metadata": {
        "id": "lWcn9Na80bIC"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step 1\n",
        "def remove_extra_cr_lf(sources, targets):\n",
        "  cleaned_sources = []\n",
        "  cleaned_targets = []\n",
        "\n",
        "  for source, target in zip(sources, targets):\n",
        "    source = re.sub(r'[\\r\\n]+', ' ', source).strip()\n",
        "    target = re.sub(r'[\\r\\n]+', ' ', target).strip()\n",
        "\n",
        "    if source and target:\n",
        "      cleaned_sources.append(source)\n",
        "      cleaned_targets.append(target)\n",
        "\n",
        "  return cleaned_sources, cleaned_targets\n"
      ],
      "metadata": {
        "id": "TXduCcCAputy"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_control_characters_and_normalize_whitespace(sources, targets):\n",
        "  cleaned_sources = []\n",
        "  cleaned_targets = []\n",
        "\n",
        "  for source, target in zip(sources, targets):\n",
        "    source = re.sub(r'\\s+', ' ', source).strip()\n",
        "    target = re.sub(r'\\s+', ' ', target).strip()\n",
        "\n",
        "    if source and target:\n",
        "      cleaned_sources.append(source)\n",
        "      cleaned_targets.append(target)\n",
        "\n",
        "  return cleaned_sources, cleaned_targets"
      ],
      "metadata": {
        "id": "h2TfeyiCYQ-0"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step 2\n",
        "def remove_empty_segments(sources, targets):\n",
        "  filtered_sources = []\n",
        "  filtered_targets = []\n",
        "\n",
        "  for source, target in zip(sources, targets):\n",
        "      if source and target and source.strip() and target.strip():\n",
        "          filtered_sources.append(source)\n",
        "          filtered_targets.append(target)\n",
        "\n",
        "  return filtered_sources, filtered_targets"
      ],
      "metadata": {
        "id": "Re95hkZxuHR2"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step 3\n",
        "import html\n",
        "def normalize_escaped_entities_list(sources, targets):\n",
        "  def normalize_escaped_entities(text):\n",
        "    if not text:\n",
        "        return text\n",
        "\n",
        "    text = re.sub(r\"&(\\d+);\", r\"&#\\1;\", text)\n",
        "\n",
        "    text = re.sub(r\"&\\\\#\", \"&#\", text)\n",
        "\n",
        "    text = html.unescape(text)\n",
        "\n",
        "    helper = r\"(?:^|\\W)(&#?[A-Za-z0-9]+;?|&#?[a-zA-Z0-9]+)\"\n",
        "\n",
        "    text = re.sub(helper, \"\", text)\n",
        "\n",
        "    return html.unescape(text)\n",
        "\n",
        "  return [normalize_escaped_entities(s) for s in sources], [normalize_escaped_entities(t) for t in targets]"
      ],
      "metadata": {
        "id": "ZzOxT37HxMvv"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "def remove_duplicate_segements(source, target):\n",
        "    seen = set()\n",
        "    cleaned_source = []\n",
        "    cleaned_target = []\n",
        "\n",
        "    for s, t in zip(source, target):\n",
        "        s_clean = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", s).lower().strip()\n",
        "        t_clean = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", t).lower().strip()\n",
        "\n",
        "        pair_key = (s_clean, t_clean)\n",
        "\n",
        "        if pair_key not in seen:\n",
        "            seen.add(pair_key)\n",
        "            cleaned_source.append(s)\n",
        "            cleaned_target.append(t)\n",
        "\n",
        "    return cleaned_source, cleaned_target"
      ],
      "metadata": {
        "id": "LV1M_zbfGLtB"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_tags(source, target):\n",
        "  tag_remover = r\"<.+?>\"\n",
        "  tag_with_only_digit_remover = r\"<.+?>\\d+<.+?>\"\n",
        "  div_remover = r\".<div.+\"\n",
        "  weird_tag = r\".+/>\"\n",
        "  two_weird_signs = r\"<>\"\n",
        "\n",
        "  new_source, new_target = [], []\n",
        "  for s, t in zip(source, target):\n",
        "    s_cleaned = re.sub(tag_with_only_digit_remover, \"\", s)\n",
        "    t_cleaned = re.sub(tag_with_only_digit_remover, \"\", t)\n",
        "\n",
        "    s_cleaned = re.sub(tag_remover, \"\", s_cleaned)\n",
        "    t_cleaned = re.sub(tag_remover, \"\", t_cleaned)\n",
        "\n",
        "    s_cleaned = re.sub(div_remover, \"\", s_cleaned)\n",
        "    t_cleaned = re.sub(div_remover, \"\", t_cleaned)\n",
        "\n",
        "    s_cleaned = re.sub(weird_tag, \"\", s_cleaned)\n",
        "    t_cleaned = re.sub(weird_tag, \"\", t_cleaned)\n",
        "\n",
        "    s_cleaned = re.sub(two_weird_signs, \"\", s_cleaned)\n",
        "    t_cleaned = re.sub(two_weird_signs, \"\", t_cleaned)\n",
        "\n",
        "    new_source.append(s_cleaned.strip())\n",
        "    new_target.append(t_cleaned.strip())\n",
        "\n",
        "  return new_source, new_target\n",
        "\n"
      ],
      "metadata": {
        "id": "koDgPvcf4_3a"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_equal_segments(source, target):\n",
        "    cleaned_sources = []\n",
        "    cleaned_targets = []\n",
        "\n",
        "    for s, t in zip(source, target):\n",
        "        s_clean = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", s).lower().strip()\n",
        "        t_clean = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", t).lower().strip()\n",
        "\n",
        "        if s_clean != t_clean:\n",
        "            cleaned_sources.append(s)\n",
        "            cleaned_targets.append(t)\n",
        "\n",
        "    return cleaned_sources, cleaned_targets"
      ],
      "metadata": {
        "id": "epnOsBOEYI5S"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_mostly_non_text_segments(source, target):\n",
        "  cleaned_sources = []\n",
        "  cleaned_targets = []\n",
        "\n",
        "  text_helper = r\"[A-Za-z\\uAC00-\\uD7A3]\";\n",
        "\n",
        "  threshold = 0.5\n",
        "\n",
        "  for s, t in zip(source, target):\n",
        "    s_text_chars = len(re.findall(text_helper, s))\n",
        "    t_text_chars = len(re.findall(text_helper, t))\n",
        "    s_total_chars = len(s.strip())\n",
        "    t_total_chars = len(t.strip())\n",
        "\n",
        "    if s_total_chars == 0 or t_total_chars == 0:\n",
        "      continue\n",
        "\n",
        "    s_ratio = s_text_chars / s_total_chars\n",
        "    t_ratio = t_text_chars / t_total_chars\n",
        "\n",
        "    if s_ratio >= threshold and t_ratio >= threshold:\n",
        "      cleaned_sources.append(s)\n",
        "      cleaned_targets.append(t)\n",
        "\n",
        "  return cleaned_sources, cleaned_targets\n"
      ],
      "metadata": {
        "id": "jbeV4WBDbYYC"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_brackets_helper(text):\n",
        "    helpermap = {\")\": \"(\", \"]\": \"[\", \"}\": \"{\", \">\": \"<\"}\n",
        "    opener_brackets = set(helpermap.values())\n",
        "    stack = []\n",
        "    to_remove = set()\n",
        "\n",
        "    for i, ch in enumerate(text):\n",
        "        if ch in opener_brackets:\n",
        "            stack.append((ch, i))\n",
        "        elif ch in helpermap:\n",
        "            if stack and stack[-1][0] == helpermap[ch]:\n",
        "                stack.pop()\n",
        "            else:\n",
        "                to_remove.add(i)\n",
        "\n",
        "    to_remove.update(i for _, i in stack)\n",
        "    return \"\".join(ch for i, ch in enumerate(text) if i not in to_remove)\n",
        "\n",
        "def remove_unbalanced_brackets(source, target):\n",
        "  cleaned_sources = []\n",
        "  cleaned_targets = []\n",
        "\n",
        "  remove_brackets_regex = r\"[\\(\\)\\[\\]\\{\\}\\<\\>]\"\n",
        "\n",
        "  for s, t in zip(source, target):\n",
        "    cleaned_sources.append(remove_brackets_helper(s))\n",
        "    cleaned_targets.append(remove_brackets_helper(t))\n",
        "\n",
        "  return cleaned_sources, cleaned_targets\n",
        ""
      ],
      "metadata": {
        "id": "XqtxGYTChQ2A"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_too_long(source, target):\n",
        "  cleaned_sources = []\n",
        "  cleaned_targets = []\n",
        "\n",
        "  max_amount = 100\n",
        "\n",
        "  for s, t in zip(source, target):\n",
        "    if len(s.split()) <= max_amount and len(t.split()) <= max_amount:\n",
        "      cleaned_sources.append(s)\n",
        "      cleaned_targets.append(t)\n",
        "\n",
        "  return cleaned_sources, cleaned_targets\n"
      ],
      "metadata": {
        "id": "gj6Q640Gumt7"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_too_short(source, target):\n",
        "  cleaned_sources = []\n",
        "  cleaned_targets = []\n",
        "\n",
        "  min_amount = 3\n",
        "\n",
        "  for s, t in zip(source, target):\n",
        "    if len(s.split()) >=min_amount and len(t.split()) >=min_amount:\n",
        "      cleaned_sources.append(s)\n",
        "      cleaned_targets.append(t)\n",
        "\n",
        "  return cleaned_sources, cleaned_targets"
      ],
      "metadata": {
        "id": "JMLxhx3JvuzG"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_ratio_check(source, target):\n",
        "  cleaned_sources = []\n",
        "  cleaned_targets = []\n",
        "\n",
        "  ratio = 0.3\n",
        "\n",
        "  for s, t in zip(source, target):\n",
        "    len_s = len(s)\n",
        "    len_t = len(t)\n",
        "    if min(len_s, len_t) / max(len_s, len_t) >= ratio:\n",
        "      cleaned_sources.append(s)\n",
        "      cleaned_targets.append(t)\n",
        "  return cleaned_sources, cleaned_targets"
      ],
      "metadata": {
        "id": "4bv_-0n9zgWS"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_quotes(source, target):\n",
        "  single_quote = r'[‘’‚‛‹›´`]'\n",
        "  double_quote = r'[«»„“„”「」﹁﹂﹃﹄]'\n",
        "\n",
        "  cleaned_sources = []\n",
        "  cleaned_targets = []\n",
        "\n",
        "  for s, t in zip(source, target):\n",
        "    normalized_source_double = re.sub(double_quote, '\"', s)\n",
        "    normalized_target_double = re.sub(double_quote, '\"', t)\n",
        "\n",
        "    normalized_source = re.sub(single_quote, \"'\", normalized_source_double)\n",
        "    normalized_target = re.sub(single_quote, \"'\", normalized_target_double)\n",
        "\n",
        "    cleaned_sources.append(normalized_source)\n",
        "    cleaned_targets.append(normalized_target)\n",
        "\n",
        "  return cleaned_sources, cleaned_targets"
      ],
      "metadata": {
        "id": "ypuQu2wlDVQv"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_up_weird_stuff_extra(source, target):\n",
        "  cleaned_sources = []\n",
        "  cleaned_targets = []\n",
        "\n",
        "  url_pattern = r\"(https?://\\S+|www\\.\\S+|[\\w-]+\\.[a-z]{2,}\\S*)\"\n",
        "\n",
        "  valid_start = re.compile(r\"^[0-9A-Za-z가-힣]\")\n",
        "\n",
        "  for s, t in zip(source, target):\n",
        "    new_s = re.sub(r\"\\\\([:;,.!?])\", r\"\\1\", s)\n",
        "    new_t = re.sub(r\"\\\\([:;,.!?])\", r\"\\1\", t)\n",
        "\n",
        "    new_s = re.sub(url_pattern, \"\", new_s, flags=re.IGNORECASE)\n",
        "    new_t = re.sub(url_pattern, \"\", new_t, flags=re.IGNORECASE)\n",
        "\n",
        "    new_s = re.sub(r\"®\", \"\", new_s)\n",
        "    new_t = re.sub(r\"®\", \"\", new_t)\n",
        "\n",
        "    new_s = re.sub(r\"©\", \"\", new_s)\n",
        "    new_t = re.sub(r\"©\", \"\", new_t)\n",
        "\n",
        "    new_s = re.sub(r\"%(?!\\d)\", \"\", new_s)\n",
        "    new_t = re.sub(r\"%(?!\\d)\", \"\", new_t)\n",
        "\n",
        "    new_s = re.sub(r\"\\$\\{[^}]+\\}\", \"\", new_s)\n",
        "    new_t = re.sub(r\"\\$\\{[^}]+\\}\", \"\", new_t)\n",
        "\n",
        "    new_s = re.sub(r\"^[^0-9A-Za-z가-힣'\\\"]+\", \"\", new_s)\n",
        "    new_t = re.sub(r\"^[^0-9A-Za-z가-힣'\\\"]+\", \"\", new_t)\n",
        "\n",
        "    new_s = re.sub(r\"^\\s*([0-9a-zA-Z]+)\\.\\s*\", \"\", new_s)\n",
        "    new_t = re.sub(r\"^\\s*([0-9a-zA-Z]+)\\.\\s*\", \"\", new_t)\n",
        "\n",
        "    cleaned_sources.append(new_s)\n",
        "    cleaned_targets.append(new_t)\n",
        "\n",
        "  return cleaned_sources, cleaned_targets"
      ],
      "metadata": {
        "id": "EicKR5QxQCFo"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_unbalanced_double_quotes(text):\n",
        "    stack = []\n",
        "    to_remove = set()\n",
        "\n",
        "    for i, ch in enumerate(text):\n",
        "        if ch == '\"':\n",
        "            if stack:\n",
        "                stack.pop()\n",
        "            else:\n",
        "                stack.append(i)\n",
        "    to_remove.update(stack)\n",
        "\n",
        "    return \"\".join(ch for i, ch in enumerate(text) if i not in to_remove)\n",
        "\n",
        "def remove_unbalanced_double_quotes_batch(source, target, only_korean=False):\n",
        "    cleaned_sources = []\n",
        "    cleaned_targets = []\n",
        "\n",
        "    for s, t in zip(source, target):\n",
        "        s_cleaned = remove_unbalanced_double_quotes(s)\n",
        "\n",
        "        t_cleaned = remove_unbalanced_double_quotes(t)\n",
        "\n",
        "        cleaned_sources.append(s_cleaned)\n",
        "        cleaned_targets.append(t_cleaned)\n",
        "\n",
        "    return cleaned_sources, cleaned_targets"
      ],
      "metadata": {
        "id": "qp4_fAFuJLrQ"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_source, clean_target = remove_extra_cr_lf(full_source, full_target)\n",
        "clean_source, clean_target = normalize_escaped_entities_list(clean_source, clean_target)\n",
        "clean_source, clean_target = remove_tags(clean_source, clean_target)\n",
        "clean_source, clean_target = clean_up_weird_stuff_extra(clean_source, clean_target)\n",
        "clean_source, clean_target = remove_unbalanced_brackets(clean_source, clean_target)\n",
        "clean_source, clean_target = normalize_quotes(clean_source, clean_target)\n",
        "clean_source, clean_target = remove_unbalanced_double_quotes_batch(clean_source, clean_target)\n",
        "clean_source, clean_target = remove_control_characters_and_normalize_whitespace(clean_source, clean_target)\n",
        "clean_source, clean_target = remove_duplicate_segements(clean_source, clean_target)\n",
        "clean_source, clean_target = remove_equal_segments(clean_source, clean_target)\n",
        "clean_source, clean_target = remove_mostly_non_text_segments(clean_source, clean_target)\n",
        "clean_source, clean_target = remove_too_long(clean_source, clean_target)\n",
        "clean_source, clean_target = remove_too_short(clean_source, clean_target)\n",
        "clean_source, clean_target = remove_ratio_check(clean_source, clean_target)\n",
        "clean_source, clean_target = remove_empty_segments(clean_source, clean_target)\n",
        "\n",
        "print(len(clean_source))\n",
        "print(len(clean_target))\n",
        "#214255-926\n",
        "#212866\n",
        "#3603\n",
        "#211332"
      ],
      "metadata": {
        "id": "rTlVdnKr-aEv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1723fae-2434-4470-fb4b-e8004fe348e4"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "211344\n",
            "211344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"source.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(clean_source))\n",
        "\n",
        "with open(\"target.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(clean_target))"
      ],
      "metadata": {
        "id": "zevEi9vz0rws"
      },
      "execution_count": 176,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}